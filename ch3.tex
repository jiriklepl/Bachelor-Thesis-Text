\chapter{CHM compiler}

Our implementation of the CHM compiler uses preprocessing and postprocessing by the GCC compiler with the intermediate representation being the C language. GCC runs the preprocessor and then compiles the fully instantiated code.

This allows us to focus on polymorphism, type inference, and type checking instead of dealing with actually interpreting the code, but that brings its toll on the efficiency of the whole process and a slightly different approach should be considered in future works.

\section{Language extensions}

The C language is extended by type variables so we can define polytyped functions and structures and by type classes which offer a way of controlled ad-hoc polymorphism. Type classes allow for defining specific (more efficient) implementations to generic algorithms or for restricting types of parameters.

We will borrow the application of these extensions from the haskell language, however, the two languages in question are very different. In haskell there is a clear distinction between variables and non-variables and between values and types but in C we do not have this distinction grounded in the language and we want to preserve it this way as much as possible. So we will require every type variable be declared before it is used (like in the listing \ref{lst:TypeClass1}).

So to preserve the syntactic principles of the C language we will use a syntax similar to languages like C++, Java, C\#, etc. Those languages have become very popular and so it should not be a bad choice.

\begin{listing}
\caption{CHM Type Class with an Implicit Type Parameter}
\label{lst:TypeClass1}
\begin{lstlisting}
<a>
class Cloneable
{
    a *clone(a *);
}
\end{lstlisting}
\end{listing}

\begin{listing}
\caption{CHM Type Class with an Explicit Type Parameter}
\label{lst:TypeClass2}
\begin{lstlisting}
<a>
class Cloneable<a>
{
    a *clone(a *);
}
\end{lstlisting}
\end{listing}

\begin{listing}
\caption{CHM Type Class Instance}
\label{lst:Instance}
\begin{lstlisting}
instance Cloneable<int>
{
    int *clone(int *value)
    {
        int *copy = new();
        // new is a generic function replacing malloc
        *copy = value;
        return copy;
    }
}
\end{lstlisting}
\end{listing}

\begin{listing}
\caption{CHM Polytype Function}
\label{lst:Function}
\begin{lstlisting}
<a>
a shallow_copy(a value)
{
    return value;
}
\end{lstlisting}
\end{listing}

\subsection{Polytyped Definitions}

% TODO: https://mail.haskell.org/pipermail/beginners/2010-December/006057.html

\subsubsection{CHM Headers}
In the listing \ref{lst:TypeClass1} we see the part \lstinline{<a>}, declaration of the type variable \lstinline{a}, similar to C++ templates and Java generics, always preceding the rest of the definition and with a scope local to the definition. This part, a \lstinline{chm_header}, has the following grammar rule:

\begin{defn}[CHM header]
    $$\text{\lstinline{chm_header}} := \text{\lstinline{<type_decls>}}\ |\ \text{\lstinline{<type_decls : constraints>}}\ |\ \text{\lstinline{< : constraints>}}$$
    where the \lstinline{type_decls} is a list of declared type variables; \lstinline{constraints} is a list of predicates the type variables have to satisfy and new type definitions (which are very similar to typedefs).
\end{defn}

The \lstinline{constraints} follow principles similar to haskell. Notable use specific to CHM is they allow for

\subsubsection{Type Classes, Their Instances and Structs}

The listing \ref{lst:TypeClass1} shows how we would define a simple \textbf{type class}, here we declare only one type we use in declarations of the class's methods and so it is implicitly considered its type parameter as well. The type parameter can be specified explicitly following the name of the class which we can see in the listing \ref{lst:TypeClass2}.

We will then use a similar syntax for application of types to the class's \textbf{instances}, see the listing \ref{lst:Instance}.

For \textbf{structs} we will use the same syntax as we use for type classes.

\subsubsection{Functions}

Functions will use the very same syntax (see the listing \ref{lst:Function}), however with slightly different behavior, they do not have any type parameters and instead they have type schemes such that their qualified types (containing the predicates specified following the type declarations) are quantified over the declared types.

\section{Parsing}

Parsing of the code is handled by the language-c project \cite{visq2018language-c}, originally meant for C99 (and later for C11), which is slightly modified into language-chm to support the extensions we introduce.

This project is used for the output of pretty-printed C code as well, this output is then inputted to GCC for finalizing the compilation.

\section{Type Inference}

For the type inference, we use the thih project \cite{jones1999typing}, but using it sensibly required some modifications to be made to it as the original thih project is not meant to be used as an efficient implementation.

Using thih required a significant pre/post-processing of the code, described in the previous chapter. % TODO: modelling

All type inference principles are explained in the first chapter. The basis for the implementation is then explained in Typing Haskell in Haskell \cite{jones1999typing}.

\section{Code Processing}

The main parts of preprocessing the code are in the AST-to-THIH project, postprocessing is then in the CHM-instantiate project which also passes the code to AST-to-THIH for preprocessing. Both of these projects are specifically built for this thesis as part of the CHM compiler.

\subsection{Monomorphization and Instantiation}

The process of instantiation of functions begins with monotype functions and then continues recursively when we determine the monotypes of all occurrences of polytype functions called from them. Similarly for types (and namely structs).

We will describe the process only on functions, but the principles of it carry to structures as well.

We start with monotype functions as we can already determine their monomorphic type. We replace all polymorphic symbols (functions, structs, and types) in their definitions with unique symbols (distinct even if the original symbols were the same), each assigned a new unique type variable.

We run the type inference algorithm on this rewritten definition and then if we successfully determined each monotype we instantiate each of the original symbols given the new inferred type (unless we have done so already - this implies we have to keep a record of all instances).

Instantiation of polytyped functions is done by simply replacing the quantified type variables with the inferred types. From there the process repeats for each new instance as if the instance was a regular monomorphic function. Type class methods are an exception to this which we will discuss in the very next part, for them there is one more step before we instantiate them.

\subsection{Type Class Method Instantiation}

All type classes are just language constructs, they are not manifested in the resulting code in any shape or form and thus they count as a zero abstraction feature. There are some built-in type classes (for example, \lstinline{Add} which states that two variables of a type that satisfies it can be added), implicit type classes for each field of a structure (and a union) and then user-defined type classes, showed before in this chapter. The implemented built-in and implicit type classes are mainly for demonstration and they are subject to change in the future.

Definitions of type classes are stored in the compiler as lists of their methods. Methods act like regular polytypes until we instantiate them.

When a method is instantiated the compiler checks its instances and determines which one (and if any) matches the inferred type. Then it instantiates this one method instance just as if it was the only definition, note that type class instances are required not to overlap because of this step of instantiation.

% FIXME: continue from here...

\subsection{Record Fields}

Implementing record fields (struct members) was challenging as the way record fields work in C has no real equivalent in the context of functional programming, although haskell's object notation or ML's records come pretty close to it. % TODO: link haskell and ML?

Record fields are implemented in such a way that all fields of a certain name have to share the exact same type. This is a slightly restricted requirement of each field having a unique name.

The reason for such requirements is that accessing a field is a function taking the record and returning the field. The record has to have this field, which is easily solved by a type class (parametrized by the record's type and named after the field) the compiler implicitly generates when the field is defined. This type class has exactly one method which is the accessor of the given field.

This approach perfectly matches the syntax of C where we know the name of the field and the variable which is supposed to have such a field. The field's type is not specified when we access it so it has to be inferable from the variable alone. And the easiest way to satisfy that is to require each field have a unique name (this is how it is implemented in haskell and ML, for example).

\subsection{Recursion limits}

It is indeterminable whether the instantiation of something continues forever. We can prove this using post correspondence problem % FIXME: prove it

So there is a limit of type depth set to 500 (this limit should not affect any sensible program, but if need be, it can be changed using the \lstinline[language=sh]{--depth} option). If some type exceeds this limit, a compilation error will be outputted stating what the compiler tried to instantiate at that time.

\subsection{Comparison to C++ template instantiation}

One could say this project just mirrors template instantiation from C++, but any such resemblance is just superficial, C++ template instantiation is not strict in type signatures and different instances can have
non-matching types.

One example of this would be:

% FIXME: continue from here

\begin{lstlisting}
template<typename>
struct function_type;

template<>
struct function_type<int> { using return_type = int; using parameter_type = int; };

template<>
struct function_type<int*> { using return_type = int; using parameter_type = int*; };

template<>
struct function_type<int**> { using return_type = int*; using parameter_type = int; };

template<typename T>
typename function_type<T>::return_type function(typename function_type<T>::parameter_type);

template<>
int function<int>(int value) { return value; }

template<>
int function<int*>(int *value) { return *value; }

template<>
int *function<int**>(int value) { return &value; }
\end{lstlisting}

C++ allows different instances to have the same type signature which can make any type inference or type deduction impossible.

One more difference is that C++ allows function overloading. % TODO: write about the paper linked by mirek

The instantiation in CHM follows inferred types by the HM type system.

\section{Code output}

The CHM compiler outputs gcc-compilable code which interprets the CHM code without any runtime overhead. % TODO: or at least I hope so

There are options allowing the user to see the inner haskell representation of the CHM code (it blurs some of the semantics unnecessary for type inference and leaves others to facilitate the readability of it), this can be used for debugging the compiler implementations. % FIXME: list the options (unimplemented yet)

It should be noted that names of symbols of polytype functions actually used in the C interpretation of the CHM code do not match their CHM counterparts which currently complicates debugging of CHM programs (this however can be solved easily on the debugger side).
